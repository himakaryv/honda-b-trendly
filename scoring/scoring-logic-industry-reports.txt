Scoring Framework for Industry Report-Based Trend Analysis
Credibility Scoring (Weight: 30%) X Frequency Scoring (Weight: 25%) X Recency Scoring (Weight: 25%) X Recency Scoring (Weight: 25%)


Credibility = (Source Tier × 0.5) + (Methodology × 0.3) + (Author Authority × 0.2)
Frequency = (Cross-Report Validation × 0.5) + (Volume × 0.25) + (Market Attention × 0.25)
Recency = (Timeline × 0.5) + (Lifecycle Position × 0.3) + (Update Velocity × 0.2)
Relevance = (Industry Specificity × 0.5) + (Actionability × 0.3) + (Market Impact × 0.2)

1. Credibility Scoring (Weight: 30%)
Credibility = (Source Tier × 0.5) + (Methodology × 0.3) + (Author Authority × 0.2)
Report Source Tier Assessment (0-10 scale):
* Tier 1 (9-10 points): Premium research firms (Gartner, McKinsey, BCG, Forrester, IDC)
* Tier 2 (7-8 points): Specialized industry analysts (CB Insights, PitchBook, Bloomberg Intelligence, S&P Global)
* Tier 3 (5-6 points): Trade associations, Big 4 consulting reports, investment bank research
* Tier 4 (3-4 points): Vendor-sponsored research, regional consultancies, market research aggregators
* Tier 5 (1-2 points): Unknown firms, self-published reports, content marketing disguised as research
Methodology Transparency Indicators:
* Clear sample size and demographics: +2 bonus
* Named data sources and collection methods: +2 bonus
* Statistical confidence intervals provided: +1 bonus
* Peer review or industry validation: +1 bonus
* Updated annually with trend tracking: +1 bonus
* Conflicts of interest disclosed: +1 bonus
Author/Analyst Authority:
* Lead analyst with >10 years domain expertise: +2 bonus
* Multiple expert contributors: +1 bonus
* Industry practitioner involvement: +1 bonus
* Academic collaboration: +1 bonus
* Quoted by major media outlets: +1 bonus


2. Frequency Scoring (Weight: 25%)
Frequency = (Cross-Report Validation × 0.5) + (Volume × 0.25) + (Market Attention × 0.25)
Cross-Report Validation Metrics:
* Universal consensus (10 points): >80% of reports mention the trend
* Strong agreement (8 points): 60-80% mention rate
* Moderate presence (6 points): 40-60% mention rate
* Emerging theme (4 points): 20-40% mention rate
* Isolated mention (2 points): <20% mention rate
Report Volume Indicators:
* 20 reports covering the trend in past year: +3 bonus
* Dedicated reports (entire report on single trend): +2 bonus
* Multiple report series tracking the trend: +2 bonus
* Quarterly updates published: +1 bonus
Market Attention Metrics:
* Featured in "Top 10" predictions by multiple firms: +2 bonus
* Dedicated conference tracks/webinars: +1 bonus
* C-suite survey priority (top 3 concern): +2 bonus
* Media coverage amplification: +1 bonus
3. Recency Scoring (Weight: 25%)
Recency = (Timeline × 0.5) + (Lifecycle Position × 0.3) + (Update Velocity × 0.2)
Publication Timeline:
* Real-time insights (10 points): Reports less than 3 months old
* Current (8 points): 3-6 months old
* Recent (6 points): 6-12 months old
* Dated but relevant (4 points): 12-18 months old
* Historical context (2 points): 18+ months old
Trend Lifecycle Position:
* First-time appearance in major reports: +3 bonus (truly emerging)
* Upgraded from "emerging" to "mainstream": +2 bonus
* Accelerating adoption metrics shown: +2 bonus
* Revised projections (upward) from previous reports: +2 bonus
* Featured in "surprising findings" sections: +1 bonus
Update Velocity:
* Monthly data updates available: +2 bonus
* Real-time dashboards provided: +2 bonus
* Forecast revisions within quarter: +1 bonus
* Breaking news addendums: +1 bonus
4. Relevance Scoring (Weight: 20%)
Relevance = (Industry Specificity × 0.5) + (Actionability × 0.3) + (Market Impact × 0.2)
Industry Specificity:
* Exact match (10 points): Report specifically for your industry/sub-sector
* Direct relevance (8 points): Your industry is primary focus
* Strong relevance (6 points): Your industry featured as major use case
* Moderate relevance (4 points): Applicable but requires adaptation
* Tangential (2 points): Lessons must be extrapolated
Actionability Indicators:
* Implementation roadmaps provided: +2 bonus
* ROI calculations included: +2 bonus
* Case studies from similar companies: +2 bonus
* Vendor landscape/solutions mapped: +1 bonus
* Regulatory considerations addressed: +1 bonus
* Risk assessments included: +1 bonus
Market Impact Projections:
* Market size/CAGR projections: +2 bonus
* Job impact/skills gap analysis: +1 bonus
* Competitive advantage quantified: +2 bonus
* Timeline to mainstream adoption: +1 bonus
* Investment requirements estimated: +1 bonus



Step 2: Source Identification Strategy
Paid/Premium Sources (High Credibility):
* Gartner: Magic Quadrants, Hype Cycles, Predicts reports
* Forrester: Waves, TechRadar, Predictions
* IDC: MarketScape, FutureScape, Trackers
* McKinsey: Global Institute reports, Industry insights
* BCG: Industry perspectives, Megatrends
* Deloitte: Tech Trends, Industry outlooks
Free/Accessible Sources (Mixed Credibility):
* Industry associations: Often high-quality sector-specific insights
* Government/NGO: OECD, World Bank, UN agency reports
* Think tanks: Brookings, RAND, specialized institutes
* Tech vendors: IBM, Microsoft, Google thought leadership (note bias)
* Investment firms: Annual outlook reports, thematic research
Step 4: Advanced Report Analysis Techniques
Consensus vs. Contrarian Analysis:
Consensus Score = (Number of agreeing reports / Total reports) × 10
- High consensus (>80%): Validated but potentially priced in
- Moderate consensus (50-80%): Sweet spot for competitive advantage
- Low consensus (<50%): Higher risk but potentially higher reward
Projection Confidence Assessment:
Confidence Score = (Methodology rigor × Historical accuracy × Source diversity) / 3
- Compare past predictions to actual outcomes
- Weight sources by track record
Hype Cycle Positioning:
For Gartner-style analysis:
- Innovation Trigger: High risk, high reward (Score × 0.7)
- Peak of Inflated Expectations: Likely overvalued (Score × 0.5)
- Trough of Disillusionment: Potential opportunity (Score × 0.8)
- Slope of Enlightenment: Validation phase (Score × 0.9)
- Plateau of Productivity: Proven but commoditizing (Score × 0.6)
Step 5: Cross-Report Synthesis Dashboard
Create a comprehensive trend tracking matrix:
Trend
	Gartner
	Forrester
	McKinsey
	IDC
	Consensus
	Avg. Score
	Signal
	Generative AI
	9.5
	9.0
	9.2
	8.8
	95%
	9.1
	STRONG
	Quantum Computing
	6.0
	5.5
	7.0
	6.5
	60%
	6.3
	MONITOR
	Edge Computing
	8.0
	8.5
	7.5
	8.2
	85%
	8.1
	INVEST
	Step 6: Quality Filters and Bias Detection
Vendor Bias Detection:
"This report is sponsored by [vendor name].
Findings include: [key points]


Identify potential biases:
- Which findings favor the sponsor?
- What alternative views are missing?
- How should I adjust my interpretation?
Rate bias risk 1-10."
Geographic/Cultural Bias Check:
- Is the research primarily from one region?
- Are cultural factors considered?
- Will findings translate to your market?
Temporal Bias Assessment:
- Are reports clustered around specific events?
- Do they reflect temporary conditions?
- What structural vs. cyclical trends are identified?
Practical Example
Evaluating "Autonomous Supply Chain" from Industry Reports:
1. Credibility (8.2/10):
   * Featured in Gartner, McKinsey, Deloitte reports (9/10 source)
   * Clear methodology in 60% of reports (7/10 methodology)
   * Mix of industry and academic authors (8/10 authority)
2. Frequency (7.8/10):
   * Mentioned in 70% of supply chain reports (8/10 validation)
   * 15 dedicated reports in past year (7/10 volume)
   * Top 3 priority in C-suite surveys (8/10 attention)
3. Recency (8.5/10):
   * 40% of reports from last quarter (9/10 timeline)
   * Moved from "emerging" to "accelerating" (8/10 lifecycle)
   * Monthly updates from 2 major firms (8/10 velocity)
4. Relevance (8.7/10):
   * Direct focus on manufacturing/logistics (9/10 specificity)
   * Implementation guides included (9/10 actionability)
   * $2.3T market impact projected (8/10 impact)
Total Score: 8.3/10 - HIGH PRIORITY TREND
Key Differences from Academic/Patent Scoring
Industry Reports Unique Value:
* Market validation: Real budget allocation and adoption data
* Practitioner perspective: Based on actual implementation experience
* Competitive context: Benchmarking and peer comparisons
* ROI focus: Economic justification and business cases
* Risk assessment: Practical challenges and failure points
Red Flags in Report Analysis:
* Single-vendor echo chamber (only vendor-sponsored reports available)
* Dramatic year-over-year projection changes (suggests poor modeling)
* Missing methodology sections (likely opinion not research)
* No dissenting views (insufficient critical analysis)
* Outdated data repackaged as new (check original research dates)
Triangulation Opportunity:
Strong Signal = High scores in:
- Academic papers (innovation pipeline) +
- Patents (commercial investment) +
- Industry reports (market validation)[a]


[a]@ankitshu@andrew.cmu.edu  do we need this?